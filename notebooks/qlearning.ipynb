{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pettingzoo.classic import connect_four_v3\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Player:\n",
    "    def __init__(self, get_action) -> None:\n",
    "        self.get_action = get_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qlearning:\n",
    "    def __init__(self, exploration_factor=0.1, discount_factor=0.7, learning_rate=0.1) -> None:\n",
    "        self.exploration_factor = exploration_factor\n",
    "        self.discount_factor = discount_factor\n",
    "        self.learning_rate = learning_rate\n",
    "        self.q_table = {}\n",
    "        self.env = connect_four_v3.env()\n",
    "\n",
    "    def __create_q_table_entry(self, state):\n",
    "        name = \"\".join([str(x) for x in state.flatten()])\n",
    "        if name not in self.q_table.keys():\n",
    "            self.q_table[name] = [0 for _ in range(7)]\n",
    "        return name\n",
    "\n",
    "    def training(self, n_training_game=1000):\n",
    "        for _ in tqdm(range(n_training_game)):\n",
    "            self.env.reset()\n",
    "            self.agents = {\n",
    "                0: {\"name\": \"player_0\", \"last_state\": None, \"current_state\": None, \"reward\": 0},\n",
    "                1: {\"name\": \"player_1\", \"last_state\": None, \"current_state\": None, \"reward\": 0},\n",
    "            }\n",
    "            end = False\n",
    "\n",
    "            i = 0\n",
    "            while end is False:\n",
    "                current_agent = self.agents[i % 2][\"name\"]\n",
    "                self.env.agent_selection = current_agent\n",
    "\n",
    "                state = self.env.observe(current_agent)\n",
    "                key = self.__create_q_table_entry(state[\"observation\"])\n",
    "\n",
    "                self.agents[i % 2][\"last_state\"] = key\n",
    "\n",
    "                if random.uniform(0, 1) < self.exploration_factor:\n",
    "                    action = self.env.action_space(current_agent).sample(state[\"action_mask\"])\n",
    "                else:\n",
    "                    action = self.get_action(state)\n",
    "\n",
    "                self.env.step(action)\n",
    "                state, reward, termination, truncation, info = self.env.last()\n",
    "\n",
    "                self.agents[i % 2][\"reward\"] = reward\n",
    "\n",
    "                key = self.__create_q_table_entry(state[\"observation\"])\n",
    "                self.agents[i % 2][\"current_state\"] = key\n",
    "\n",
    "                end = termination or truncation\n",
    "\n",
    "                if end:\n",
    "                    if self.agents[i % 2][\"reward\"] == 1:\n",
    "                        self.agents[(i + 1) % 2][\"reward\"] = -1\n",
    "\n",
    "                    for j in [0, 1]:\n",
    "                        old_value = self.q_table[self.agents[j][\"last_state\"]][action]\n",
    "                        next_max = np.max(self.q_table[self.agents[j][\"current_state\"]])\n",
    "                        new_value = (1 - self.learning_rate) * old_value + self.learning_rate * (\n",
    "                            self.agents[j][\"reward\"] + self.discount_factor * next_max\n",
    "                        )\n",
    "                        self.q_table[self.agents[j][\"last_state\"]][action] = new_value\n",
    "\n",
    "                elif self.agents[(i + 1) % 2][\"last_state\"] != None:\n",
    "                    old_value = self.q_table[self.agents[(i + 1) % 2][\"last_state\"]][action]\n",
    "                    next_max = np.max(self.q_table[self.agents[(i + 1) % 2][\"current_state\"]])\n",
    "                    new_value = (1 - self.learning_rate) * old_value + self.learning_rate * (\n",
    "                        self.agents[(i + 1) % 2][\"reward\"] + self.discount_factor * next_max\n",
    "                    )\n",
    "                    self.q_table[self.agents[(i + 1) % 2][\"last_state\"]][action] = new_value\n",
    "                i += 1\n",
    "            self.env.close()\n",
    "\n",
    "    def play(self):\n",
    "        self.env = connect_four_v3.env(render_mode=\"human\")\n",
    "        self.env.reset()\n",
    "        self.agents = {\n",
    "            0: {\"name\": \"player_0\", \"last_state\": None, \"current_state\": None, \"reward\": 0},\n",
    "            1: {\"name\": \"player_1\", \"last_state\": None, \"current_state\": None, \"reward\": 0},\n",
    "        }\n",
    "        end = False\n",
    "        i = 0\n",
    "        while end is False:\n",
    "            current_agent = self.agents[i % 2][\"name\"]\n",
    "            self.env.agent_selection = current_agent\n",
    "\n",
    "            state = self.env.observe(current_agent)\n",
    "\n",
    "            action = self.get_action(state)\n",
    "\n",
    "            self.env.step(action)\n",
    "            state, reward, termination, truncation, info = self.env.last()\n",
    "\n",
    "            end = termination or truncation\n",
    "\n",
    "            i += 1\n",
    "            time.sleep(0.3)\n",
    "        self.env.close()\n",
    "\n",
    "    def get_action(self, state):\n",
    "        key = self.__create_q_table_entry(state[\"observation\"])\n",
    "        possible = [\n",
    "            self.q_table[key][i] if state[\"action_mask\"][i] != 0 else -np.inf for i in range(7)\n",
    "        ]\n",
    "        action = np.argmax(possible)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = Qlearning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 167.44it/s]\n"
     ]
    }
   ],
   "source": [
    "Q.training(n_training_game=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q.play()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "player = Player(get_action=lambda state: Q.get_action(state))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
