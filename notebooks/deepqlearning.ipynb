{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pettingzoo.classic import connect_four_v3\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size=6 * 7 * 2, output_size=7):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.stack = nn.Sequential(\n",
    "            nn.Linear(in_features=self.input_size, out_features=self.input_size**2),\n",
    "            nn.PReLU(),\n",
    "            nn.Linear(self.input_size**2, self.output_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Player:\n",
    "    def __init__(self, get_action) -> None:\n",
    "        self.get_action = get_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qlearning:\n",
    "    def __init__(\n",
    "        self, exploration_factor=0.1, discount_factor=0.7, learning_rate=0.1, model=None\n",
    "    ) -> None:\n",
    "        self.exploration_factor = exploration_factor\n",
    "        self.discount_factor = discount_factor\n",
    "        self.learning_rate = learning_rate\n",
    "        self.q_table = {}\n",
    "        self.model = model\n",
    "\n",
    "    def training(self, n_training_game=1000, batch_size=100):\n",
    "        self.env = connect_four_v3.env()\n",
    "        self.loss = nn.MSELoss()\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters())\n",
    "        for _ in tqdm(range(n_training_game // batch_size)):\n",
    "            inputs = []\n",
    "            outputs = []\n",
    "            games = 0\n",
    "            while games != batch_size:\n",
    "                self.env.reset()\n",
    "                self.agents = {\n",
    "                    0: {\n",
    "                        \"name\": \"player_0\",\n",
    "                        \"last_state\": None,\n",
    "                        \"current_state\": None,\n",
    "                        \"reward\": 0,\n",
    "                        \"action\": None,\n",
    "                    },\n",
    "                    1: {\n",
    "                        \"name\": \"player_1\",\n",
    "                        \"last_state\": None,\n",
    "                        \"current_state\": None,\n",
    "                        \"reward\": 0,\n",
    "                        \"action\": None,\n",
    "                    },\n",
    "                }\n",
    "                end = False\n",
    "\n",
    "                i = 0\n",
    "                while end is False:\n",
    "                    current_agent = self.agents[i % 2][\"name\"]\n",
    "                    self.env.agent_selection = current_agent\n",
    "\n",
    "                    state = self.env.observe(current_agent)\n",
    "\n",
    "                    self.agents[i % 2][\"last_state\"] = state[\"observation\"].flatten()\n",
    "\n",
    "                    y = self.model(torch.Tensor(state[\"observation\"].flatten()))\n",
    "\n",
    "                    if random.uniform(0, 1) < self.exploration_factor:\n",
    "                        action = self.env.action_space(current_agent).sample(state[\"action_mask\"])\n",
    "                    else:\n",
    "                        possible = [\n",
    "                            y[i].item() if state[\"action_mask\"][i] != 0 else -np.inf\n",
    "                            for i in range(7)\n",
    "                        ]\n",
    "                        action = np.argmax(possible)\n",
    "\n",
    "                    self.env.step(action)\n",
    "                    state, reward, termination, truncation, info = self.env.last()\n",
    "\n",
    "                    self.agents[i % 2][\"reward\"] = reward\n",
    "\n",
    "                    self.agents[i % 2][\"current_state\"] = state[\"observation\"].flatten()\n",
    "\n",
    "                    y = y.detach().numpy()\n",
    "\n",
    "                    end = termination or truncation\n",
    "\n",
    "                    self.agents[i % 2][\"action\"] = y\n",
    "\n",
    "                    if end:\n",
    "                        games += 1\n",
    "                        if self.agents[i % 2][\"reward\"] == 1:\n",
    "                            self.agents[(i + 1) % 2][\"reward\"] = -1\n",
    "\n",
    "                        for j in [0, 1]:\n",
    "                            try:\n",
    "                                Q_sa = torch.max(\n",
    "                                    self.model(torch.Tensor(self.agents[j][\"current_state\"]))\n",
    "                                ).item()\n",
    "                                self.agents[j % 2][\"action\"][action] = (\n",
    "                                    reward + self.discount_factor * Q_sa\n",
    "                                )\n",
    "\n",
    "                                inputs.append(self.agents[j % 2][\"last_state\"])\n",
    "                                outputs.append(self.agents[j % 2][\"action\"])\n",
    "\n",
    "                            except:\n",
    "                                pass\n",
    "                    i += 1\n",
    "\n",
    "                self.env.close()\n",
    "            inputs = torch.Tensor(np.array(inputs))\n",
    "            outputs = torch.Tensor(np.array(outputs)).reshape(\n",
    "                inputs.shape[0], 1, self.model.output_size\n",
    "            )\n",
    "\n",
    "            pred = self.model(inputs)\n",
    "            loss = self.loss(pred.unsqueeze(1), outputs)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "    def play(self):\n",
    "        self.env = connect_four_v3.env(render_mode=\"human\")\n",
    "        self.env.reset()\n",
    "        self.agents = {\n",
    "            0: {\"name\": \"player_0\", \"last_state\": None, \"current_state\": None, \"reward\": 0},\n",
    "            1: {\"name\": \"player_1\", \"last_state\": None, \"current_state\": None, \"reward\": 0},\n",
    "        }\n",
    "        end = False\n",
    "        i = 0\n",
    "        while end is False:\n",
    "            current_agent = self.agents[i % 2][\"name\"]\n",
    "            self.env.agent_selection = current_agent\n",
    "\n",
    "            state = self.env.observe(current_agent)\n",
    "\n",
    "            y = self.model(torch.Tensor(state[\"observation\"].flatten()))\n",
    "\n",
    "            possible = [y[i].item() if state[\"action_mask\"][i] != 0 else -np.inf for i in range(7)]\n",
    "            action = np.argmax(possible)\n",
    "\n",
    "            self.env.step(action)\n",
    "            state, reward, termination, truncation, info = self.env.last()\n",
    "\n",
    "            y = y.detach().numpy()\n",
    "\n",
    "            end = termination or truncation\n",
    "\n",
    "            i += 1\n",
    "            time.sleep(0.3)\n",
    "        self.env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = Qlearning(model=NeuralNetwork())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 156/156 [02:25<00:00,  1.07it/s]\n"
     ]
    }
   ],
   "source": [
    "Q.training(n_training_game=100000, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q.play()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "player = Player(get_action=lambda state: Q.get_action(state))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local-venv",
   "language": "python",
   "name": "local-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
